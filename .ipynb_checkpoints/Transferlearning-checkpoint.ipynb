{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pprint as pp\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg19 import (\n",
    "    VGG19, preprocess_input, decode_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VGG19 model\n",
    "# https://keras.io/applications/#VGG19\n",
    "model = VGG19(\n",
    "    include_top=False, \n",
    "    weights='imagenet')\n",
    "# model_original = VGG19(\n",
    "#     include_top=True, \n",
    "#     weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<keras.engine.input_layer.InputLayer object at 0x000001CB34A68390>,\n",
      " <keras.layers.convolutional.Conv2D object at 0x000001CB34A68860>,\n",
      " <keras.layers.convolutional.Conv2D object at 0x000001CB34A685C0>,\n",
      " <keras.layers.pooling.MaxPooling2D object at 0x000001CB34A68F60>,\n",
      " <keras.layers.convolutional.Conv2D object at 0x000001CB34A88C88>,\n",
      " <keras.layers.convolutional.Conv2D object at 0x000001CB34AA8E80>,\n",
      " <keras.layers.pooling.MaxPooling2D object at 0x000001CB34ABFDD8>,\n",
      " <keras.layers.convolutional.Conv2D object at 0x000001CB34AE4B38>,\n",
      " <keras.layers.convolutional.Conv2D object at 0x000001CB34AFD5C0>,\n",
      " <keras.layers.convolutional.Conv2D object at 0x000001CB34B0FEB8>,\n",
      " <keras.layers.convolutional.Conv2D object at 0x000001CB34B21908>,\n",
      " <keras.layers.pooling.MaxPooling2D object at 0x000001CB34B4B898>,\n",
      " <keras.layers.convolutional.Conv2D object at 0x000001CB34BB34E0>,\n",
      " <keras.layers.convolutional.Conv2D object at 0x000001CB34BB32B0>,\n",
      " <keras.layers.convolutional.Conv2D object at 0x000001CB34BC96D8>,\n",
      " <keras.layers.convolutional.Conv2D object at 0x000001CB34BF0940>,\n",
      " <keras.layers.pooling.MaxPooling2D object at 0x000001CB34C15DA0>,\n",
      " <keras.layers.convolutional.Conv2D object at 0x000001CB34C2DFD0>,\n",
      " <keras.layers.convolutional.Conv2D object at 0x000001CB34C3BB00>,\n",
      " <keras.layers.convolutional.Conv2D object at 0x000001CB34C54748>,\n",
      " <keras.layers.convolutional.Conv2D object at 0x000001CB34C7AD30>,\n",
      " <keras.layers.pooling.MaxPooling2D object at 0x000001CB34C8E828>]\n",
      "<tf.Tensor 'block5_pool/MaxPool:0' shape=(?, ?, ?, 512) dtype=float32>\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(model.layers)\n",
    "pp.pprint(model.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a global spatial average pooling layer\n",
    "x = model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(2, activation='softmax')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the model we will train\n",
    "model = Model(inputs=model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load image data\n",
    "import glob\n",
    "filtered_images_path= glob.glob('./imgs_with_filter/imgs_with_filter/*')\n",
    "unfiltered_imgs_path=glob.glob('./unfiltered_1083/*')\n",
    "images_path=filtered_images_path+unfiltered_imgs_path\n",
    "images_list=[]\n",
    "image_size = (299, 299)\n",
    "for img_path in images_path:\n",
    "    image_stored=image.load_img(img_path,target_size=image_size)\n",
    "    \n",
    "    # Preprocess image for model prediction\n",
    "# This step handles scaling and normalization for Xception\n",
    "    x = image.img_to_array(image_stored)\n",
    "#     x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    images_list.append(x)\n",
    "    \n",
    "# X=images_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_labels=np.full((len(filtered_images_path),2),(True,False),np.bool)\n",
    "unfiltered_labels=np.full((len(unfiltered_imgs_path),2),(False,True),np.bool)\n",
    "y=np.concatenate([filtered_labels,unfiltered_labels])\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data into test and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "def generator(features, labels, batch_size):\n",
    " # Create empty arrays to contain batch of features and labels#\n",
    " batch_features = np.zeros((batch_size, 299, 299, 3))\n",
    " batch_labels = np.zeros((batch_size,2))\n",
    " while True:\n",
    "   for i in range(batch_size):\n",
    "     # choose random index in features\n",
    "     index= random.choice(len(features),1)\n",
    "     batch_features[i] = (features[index])\n",
    "     batch_labels[i] = labels[index]\n",
    "   yield batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the model on the new data for a few epochs\n",
    "# model.fit_generator(\"\")\n",
    "model.fit(X_train,y_train,batch_size=10,epochs=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "160/160 [==============================] - 1436s 9s/step - loss: 2.0215 - val_loss: 1.7530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cb3aa62c50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "model.fit_generator(generator(X_train, y_train, 10), steps_per_epoch=160, epochs=1 , validation_data=generator(X_test,y_test,10), validation_steps=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`steps_per_epoch=None` is only valid for a generator based on the `keras.utils.Sequence` class. Please specify `steps_per_epoch` or use the `keras.utils.Sequence` class.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-41640da40ec5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# train the model on the new data for a few epochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# at this point, the top layers are well trained and we can start fine-tuning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1426\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1428\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             raise ValueError('`steps_per_epoch=None` is only valid for a'\n\u001b[0m\u001b[0;32m     53\u001b[0m                              \u001b[1;34m' generator based on the '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                              \u001b[1;34m'`keras.utils.Sequence`'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: `steps_per_epoch=None` is only valid for a generator based on the `keras.utils.Sequence` class. Please specify `steps_per_epoch` or use the `keras.utils.Sequence` class."
     ]
    }
   ],
   "source": [
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model.fit_generator(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_path):\n",
    "    image_stored=image.load_img(img_path,target_size=image_size)\n",
    "    \n",
    "    # Preprocess image for model prediction\n",
    "# This step handles scaling and normalization for Xception\n",
    "    x = image.img_to_array(image_stored)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    prediction = model.predict(x)\n",
    "    print(type(prediction))\n",
    "    print(prediction)\n",
    "#     decode_predictions(prediction, top=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[0.7798396  0.22016042]]\n"
     ]
    }
   ],
   "source": [
    "predict('./testing_images/unfiltered_4.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
